{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e6cb21",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ded8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from moge.model.v2 import MoGeModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df972429",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6fdfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed838c44",
   "metadata": {},
   "source": [
    "# Load the model from huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001659b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MoGe-2 model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Loading MoGe-2 model...\")\n",
    "model = MoGeModel.from_pretrained(\"Ruicheng/moge-2-vitl-normal\").to(device)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74775f6c",
   "metadata": {},
   "source": [
    "\n",
    "# Input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc209c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path = \"/root/moge_work/images/ test101.png\"  # Change this to your image path\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7fe3c",
   "metadata": {},
   "source": [
    "# Read the input image and convert to tensor (3, H, W) with RGB values normalized to [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e329a",
   "metadata": {},
   "source": [
    "when you write imread the cv2.imread() takes image in BGR format but most of the model wants RGB format if you dont do it then color will be swapped \n",
    "\n",
    "A red apple in BGR: [0, 0, 255] (high blue value)\n",
    "Same apple in RGB: [255, 0, 0] (high red value)\n",
    "\n",
    "for the second line \n",
    "The Doc said to normalize them and use permute like\n",
    "* PyTorch CNNs expect channel-first format: (C, H, W) \n",
    "* OpenCV/PIL use channel-last format: (H, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3eaea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image from /root/moge_work/images/ test101.png...\n",
      "Image shape: torch.Size([3, 1070, 722])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reading image from {input_path}...\")\n",
    "input_image_bgr = cv2.imread(input_path)\n",
    "if input_image_bgr is None:\n",
    "    raise ValueError(f\"Could not read image at {input_path}\")\n",
    "\n",
    "input_image_rgb = cv2.cvtColor(input_image_bgr, cv2.COLOR_BGR2RGB)\n",
    "input_image = torch.tensor(input_image_rgb / 255, dtype=torch.float32, device=device).permute(2, 0, 1)\n",
    "\n",
    "\n",
    "print(f\"Image shape: {input_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44823815",
   "metadata": {},
   "source": [
    "\n",
    "# Infer\n",
    "The part where model does it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a533e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n",
      "Inference complete!\n",
      "Output keys: dict_keys(['points', 'intrinsics', 'depth', 'mask', 'normal'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Running inference...\")\n",
    "with torch.no_grad():  # Save memory\n",
    "    output = model.infer(input_image)\n",
    "\n",
    "\n",
    "print(\"Inference complete!\")\n",
    "print(f\"Output keys: {output.keys()}\")\n",
    "# print(f\"just output {output['normal']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d228833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save outputs\n",
    "base_name = os.path.splitext(os.path.basename(input_path))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2ec81",
   "metadata": {},
   "source": [
    "## 1. Save depth map\n",
    "\n",
    "Distance from the camera to each pixel in the scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09040ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth Shape (H,W) torch.Size([1070, 722])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Depth Shape (H,W) {output['depth'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26798e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved depth map to output/ test101_depth.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_549662/1753323555.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  depth_normalized = ((depth - depth.min()) / (depth.max() - depth.min()) * 255).astype(np.uint8) # .astype(np.uint8): Converts to 8-bit integers for image format\n",
      "/tmp/ipykernel_549662/1753323555.py:2: RuntimeWarning: invalid value encountered in cast\n",
      "  depth_normalized = ((depth - depth.min()) / (depth.max() - depth.min()) * 255).astype(np.uint8) # .astype(np.uint8): Converts to 8-bit integers for image format\n"
     ]
    }
   ],
   "source": [
    "depth = output[\"depth\"].cpu().numpy() # inorder to process with numpy you need to move the tensor to cpu first else you will get an error\n",
    "depth_normalized = ((depth - depth.min()) / (depth.max() - depth.min()) * 255).astype(np.uint8) # .astype(np.uint8): Converts to 8-bit integers for image format\n",
    "depth_colored = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_INFERNO)\n",
    "cv2.imwrite(f\"{output_dir}/{base_name}_depth.png\", depth_colored)\n",
    "print(f\"Saved depth map to {output_dir}/{base_name}_depth.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff606b63",
   "metadata": {},
   "source": [
    "## 2. Save normal map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "364e1ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved normal map to output/ test101_normal.png\n"
     ]
    }
   ],
   "source": [
    "if \"normal\" in output:\n",
    "    normal = output[\"normal\"].cpu().numpy()\n",
    "    # Convert from [-1, 1] to [0, 255]\n",
    "    normal_vis = ((normal + 1) / 2 * 255).astype(np.uint8)\n",
    "    normal_vis_bgr = cv2.cvtColor(normal_vis, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(f\"{output_dir}/{base_name}_normal.png\", normal_vis_bgr)\n",
    "    print(f\"Saved normal map to {output_dir}/{base_name}_normal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3617c7",
   "metadata": {},
   "source": [
    "Surface orientation (which direction the surface is facing) at each pixel\n",
    "\n",
    "its actually in range [1 , -1] so cahanged it to 0 to 255 for img png \n",
    "\n",
    "Shape: (Height, Width, 3) - 3D unit vector per pixel\n",
    "Coordinate System: Same as points (OpenCV camera coords)\n",
    "Values: Unit vectors (length = 1) in range [-1, 1] for each component\n",
    "\n",
    "Points perpendicular to the surface\n",
    "Used to understand surface orientation\n",
    "\n",
    "\n",
    "Usage:\n",
    "\n",
    "Relighting\n",
    "Surface analysis\n",
    "Better 3D reconstruction\n",
    "Material/texture estimation\n",
    "\n",
    "\n",
    "Red channel: x-direction (left-right)\n",
    "Green channel: y-direction (up-down)\n",
    "Blue channel: z-direction (forward-backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34daf1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just output of normal torch.Size([1070, 722, 3])\n",
      "just output of a random point tensor([-0.3092,  0.0696, -0.9484], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"just output of normal {output['normal'].shape}\")\n",
    "print(f\"just output of a random point {output['normal'][289][277]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1087f",
   "metadata": {},
   "source": [
    "## 3. Save mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mask to output/ test101_mask.png\n"
     ]
    }
   ],
   "source": [
    "mask = output[\"mask\"].cpu().numpy()\n",
    "mask_vis = (mask * 255).astype(np.uint8) # It was in [0,1], convert to [0,255]\n",
    "cv2.imwrite(f\"{output_dir}/{base_name}_mask.png\", mask_vis)\n",
    "print(f\"Saved mask to {output_dir}/{base_name}_mask.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "558c9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization of mask shape: (1070, 722)\n",
      "see a random point of mask: 255\n"
     ]
    }
   ],
   "source": [
    "print(f\"Visualization of mask shape: {mask_vis.shape}\")\n",
    "\n",
    "print(f\"see a random point of mask: {mask_vis[289][277]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255a640",
   "metadata": {},
   "source": [
    "# 4. Save point cloud as PLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "679444a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = output[\"points\"].cpu().numpy()\n",
    "mask_np = mask.astype(bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5d41c",
   "metadata": {},
   "source": [
    "\n",
    "### Get valid points and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd48b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_points = points[mask_np]\n",
    "valid_colors = input_image_rgb[mask_np]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a74e47",
   "metadata": {},
   "source": [
    "# Write PLY file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f15c6",
   "metadata": {},
   "source": [
    "\n",
    "### **Complete PLY File Structure:**\n",
    "```\n",
    "ply                           ← File type\n",
    "format ascii 1.0              ← Text format\n",
    "element vertex 514136         ← Number of points\n",
    "property float x              ← Property definitions\n",
    "property float y\n",
    "property float z\n",
    "property uchar red\n",
    "property uchar green\n",
    "property uchar blue\n",
    "end_header                    ← End of header\n",
    "1.234 -0.567 3.890 255 128 64 ← Point 1: position + color\n",
    "0.123 0.456 2.789 120 200 50  ← Point 2: position + color\n",
    "-0.987 1.234 5.678 30 60 90   ← Point 3: position + color\n",
    "...                           ← 514,133 more lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab0dc65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved point cloud to output/ test101_pointcloud.ply\n"
     ]
    }
   ],
   "source": [
    "ply_path = f\"{output_dir}/{base_name}_pointcloud.ply\"\n",
    "with open(ply_path, 'w') as f:\n",
    "    f.write(\"ply\\n\")\n",
    "    f.write(\"format ascii 1.0\\n\")\n",
    "    f.write(f\"element vertex {len(valid_points)}\\n\")\n",
    "    f.write(\"property float x\\n\")\n",
    "    f.write(\"property float y\\n\")\n",
    "    f.write(\"property float z\\n\")\n",
    "    f.write(\"property uchar red\\n\")\n",
    "    f.write(\"property uchar green\\n\")\n",
    "    f.write(\"property uchar blue\\n\")\n",
    "    f.write(\"end_header\\n\")\n",
    "    for point, color in zip(valid_points, valid_colors):\n",
    "        f.write(f\"{point[0]} {point[1]} {point[2]} {color[0]} {color[1]} {color[2]}\\n\")\n",
    "\n",
    "print(f\"Saved point cloud to {ply_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fe2d6",
   "metadata": {},
   "source": [
    "  x y z red green blue <br>\n",
    "   1.2 0.5 3.4 255 128 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec03e8",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Print camera intrinsics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e874c",
   "metadata": {},
   "source": [
    "\n",
    "- **What it is**: Camera parameters that describe how 3D points project to 2D pixels\n",
    "- **Shape**: `(3, 3)` matrix\n",
    "- **Format**: Standard camera intrinsic matrix:\n",
    "```\n",
    "[[fx,  0, cx],\n",
    " [ 0, fy, cy],\n",
    " [ 0,  0,  1]]\n",
    "\n",
    "fx, fy: Focal lengths (in pixels)\n",
    "cx, cy: Principal point (usually image center)\n",
    "Usage:\n",
    "\n",
    "Converting between 3D coordinates and 2D pixels\n",
    "Camera calibration\n",
    "3D reconstruction pipelines\n",
    "Computing field of view (FOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6bc286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Camera Intrinsics:\n",
      "[[4.409797 0.       0.5     ]\n",
      " [0.       2.975583 0.5     ]\n",
      " [0.       0.       1.      ]]\n",
      "\n",
      "✅ All outputs saved successfully!\n",
      "Point map shape: torch.Size([1070, 722, 3])\n",
      "Depth map shape: torch.Size([1070, 722])\n",
      "Normal map shape: torch.Size([1070, 722, 3])\n"
     ]
    }
   ],
   "source": [
    "intrinsics = output[\"intrinsics\"].cpu().numpy()\n",
    "print(\"\\nCamera Intrinsics:\")\n",
    "print(intrinsics)\n",
    "\n",
    "print(\"\\n✅ All outputs saved successfully!\")\n",
    "print(f\"Point map shape: {output['points'].shape}\")\n",
    "print(f\"Depth map shape: {output['depth'].shape}\")\n",
    "print(f\"Normal map shape: {output['normal'].shape if 'normal' in output else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a730dcc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f419aa2a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
